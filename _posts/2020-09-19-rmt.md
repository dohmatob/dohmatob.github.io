---
layout: post
title:  "A criminal's view on random matrix theory (WIP)"
date:   2020-09-19 12:26
categories: research
images:

 - url: /assets/figures/duality/duality.png
 - alt: Fenchel-Rockafellar duality theorem, one ring to rule'em all!
 - title: Fenchel-Rockafellar duality theorem, one ring to rule'em all!

---


{% include mathjax.html %}
$$
\def\sign{\operatorname{sign}}
\def\qed{\Box}
\def\im{\operatorname{im}}
\def\grad{\operatorname{grad}}
\def\curl{\operatorname{curl}}
\def\div{\operatorname{div}}
\def\dim{\operatorname{dim}}
\def\X{\mathcal X}
\def\Y{\mathcal Y}
\def\C{\mathcal C}
\def\M{\mathcal M}
\def\Xstar{\X^\star}
\def\Ystar{\Y^\star}
\def\Astar{A^\star}
\def\BX{\mathbb B_{\X}}
\def\BXstar{\mathbb B_{\Xstar}}
\def\Xss{\X^{\star\star}}
\def\Yss{\Y^{\star\star}}
\def\gss{g^{\star\star}}
\def\fss{f^{\star\star}}
\def\fstar{f^\star}
\def\gstar{g^\star}
\def\xstar{x^\star}
\def\ystar{y^\star}
\def\opt{\operatorname{opt}}
\def\dom{\operatorname{dom}}
\def\conv{\operatorname{conv}}
\def\epi{\operatorname{epi}}
\def\iff{\;\operatorname{iff}\;}
\def\st{\;\operatorname{s.t}\;}
\def\softmax{\operatorname{softmax}}
\def\kl{\operatorname{KL}}
\def\tv{\operatorname{TV}}
\def\div{\operatorname{div}}
\def\inte{\operatorname{int}}
\def\ball{\operatorname{Ball}}
\def\card{\operatorname{card}}
\def\enet{\mathcal N_\epsilon}
\def\bP{\mathbb P}
$$


## I -- Introduction
In this post, we will be interested in the extreme singular-values of a random $N \times n$ matrix $A$ with real coefficients.
The parameter $\lambda := n/N$, usually referred to as the *aspect ratio*, will play a central role in the proofs. Singular values of $A$ are nothing other than the nonzero eigenvalues of the covariance matrix $AA^T$. It is well-known that the joint distribution of the covariance matrix of a random matrix with entries which are iid zero-mean and bounded $(3+\delta)$-moments is a *Trace-Widom* in the limit when $n,N \to \infty$ with $n/N \in (0, 1)$. This result is quite impressive, as it is (1) universal (2) gives the analytic form for the joint distribution. But, we want more! We are unsatisfied with this result for at least two good reasons

- It is asymptotic!
- It is asymptotic!

Recall the well-known variational formulae for the largest and the smallest singular-values of $A$, namely

$$
\begin{split}
s_{\max}(A) &:= \sup_{x \in \mathbb S_{n-1}}\|Ax\|,\\
s_{\min}(A) &:= \inf_{x \in \mathbb S_{n-1}}\|Ax\|,
\end{split}
$$

where $\mathbb S_{n-1} := \\{x \in \mathbb R^n\mid \\|x\\| = 1\\}$ be the unit-sphere in $n$-dimensional euclidean space $\mathbb R^n$.
Since $A$ is a random matrix, both  $s_{\min}(A)$ and $s_{\max}(A)$
are random variables. We will be interested in non-asymptotic
confidence intervals for these quantities.

The main result of this post will be the following.

>**Theorem.** Let the entries of $A$ be iid from $N(0,1)$. For every
   $C>0$, there exists $c>0$ depending on $\lambda$ and $C$ such
   that $s_{\min}(A) \ge c\sqrt{N}$ w.p $1-2e^{-CN}$.

## II -- Main ingredients

Let $(\mathcal X,d)$ be a metric space, $S \subseteq \mathcal X$, and $\epsilon > 0$. A subset $\mathcal N \subseteq X$ is called an $\epsilon$-net for $S$ if every point of $S$ is withing a distance of $\epsilon$ from a point of $\mathcal N$. That is, $S \subseteq \cup_{a \in \mathcal N}\ball(a;\epsilon)$. An $\epsilon$-net is called *maximal* if it is maximal w.r.t set inclusion. By *Zorn's Lemma*, such a net always exists. The notation $\mathcal N_\epsilon(S)$ will be used to denote any maximal $\epsilon$-net for $S$.


>**Lemma.** For every $\epsilon \in (0, 1)$, we have the bound $\card(\enet(S)) \le (3/\epsilon)^k$. More generally, if
$S \subseteq (\mathbb R^k,\ell_2)$ is centrally-symmetric, then $\card(\enet(S)) \le \mathcal O((1/\epsilon)^k)$. 

The proof is standard and can be carried out via a volume-packing argument and is left as an exercise (Hint: How many disjoint spheres of radius $\epsilon$ can you enclose with $\mathbb S_{n-1}$ ?).

A corner stone of the proof will be to establish an upper bound for $s_{\max}(A)$ which holds with probability $1-e^{-C'N}$, for any $\lambda \in [0, 1]$ and for some $C'>0$.

Let us recall some basic notions from geometric probability theory.
>**Definition.** A zero-mean random variable $\zeta$ on $\mathbb R$ is called $\sigma^2$-subGaussian if there exists a constant
$A>0$ such that
$$
\mathbb \bP(|\zeta| \ge t) \le Ae^{-t^2/(2\sigma^2)},\;\forall t \ge 0.
$$

Thus the tails of a subGaussian random variable compare to that of a Gaussian of same variance.
Usual examples include Gaussian / normal random variables; bounded random variables; etc. The following result will be crucial in the sequel.

For any positive integer $k$, let $\mathbb B_k := \{x \in \mathbb R^k \text{ s.t } \|x\| \le 1\}$ be the unit-ball in $k$-dimensional euclidean space $\mathbb R^k$ and let $\mathbb S_{k-1} := \{x \in \mathbb R^k \text{ s.t } \|x\| = 1\}$ be the corresponding unit-sphere.

>**Fact 1 (Gaussian small-ball probability).** *If $X = (X_1,\ldots,X_k) \sim \mathcal N(0,I_k)$, then the exists $C_1>0$ such that $\bP(\|X\| \le u \sqrt{k}) \le (C_1u)^k$.*

*Proof.* $\bP(\|X\| \le u \sqrt{k}) = (2\pi)^{-N/2}\int_{u\mathbb B_k}e^{-\|x\|^2}dx \le (2\pi)^{-k/2}\mbox{vol}(u\mathbb B_k) \le (C_1u)^k$, for some $C_1>0$ (which can be made explicit).

>**Fact 2 (Spectral-norm upper bound).** *For every $C>0$, there exists $C_0>0$ such that $s_{\max}(A) \le C_0\sqrt{N}$ w.p $1-e^{-CN}$.*

*Proof.* See **Fact 2.4** of this [paper by Litvak, Pajor, and Rudelson][1].

## III. -- Proof of the main claim

We are now ready to proof the main claim.

*Proof of the main claim.* Let $\epsilon \in (0, 1)$, to be prescribed
 later, and let  $\enet$ be a maximal $\epsilon$-net for
 $\mathbb S_{n-1}$. Note that $|\enet| \le (3/\epsilon)^n$. Now, for
 each $x \in \mathbb S_{n-1}$, there exists $z \in \enet$ such that
 $\\|x-z\\| \le \epsilon$. Writing $x = x + (x -z)$, the triangle
 inequality gives

$$
\|Ax\| \ge \|Az\| - \|A(x-z)\| \ge \|Az\|-\epsilon s_\max(A).
$$

Minimizing both sides, we obtain

$$
s_\min(A) = \inf_{x \in \mathbb S_{n-1}}\|Ax\| \ge \min_{z \in \enet}\|Az\|-\epsilon s_\max(A).\tag{1}
$$

Let $C_1$ be as in **Fact 1** with $k=N$. For arbitrary $C>0$, let $C_0$ be as in **Fact 2** with $k=N$, and let $c > 0$, to be carefully chosen later. By (1),  we know that if $s_\max(A) \le C_0\sqrt{N}$ and  $\|Az\| \ge 2c \sqrt{N}$ for all $z \in \enet$, then $s_\min(A) \ge 2c\sqrt{N}-\epsilon C_0\sqrt{N} \ge c\sqrt{N}$ when $\epsilon = c/C_0$ with $c <  C_0$. Thus,

$$
\begin{split}
\bP(s_\min(A) < c\sqrt{N}) &= \bP(s_\min(A) < c \sqrt{N},s_\max(A) > C_0\sqrt{N})\\
&\quad\quad\quad + \bP(s_\min(A) > C_0\sqrt{N},s_\max(A) \le C_0\sqrt{N})\\
&\le \bP(s_\max(A) > C_0\sqrt{N}) + \bP(s_\min(A) < c \sqrt{N},s_\max(A) \le C_0\sqrt{N})\\
& \le e^{-CN} + \bP(\min_{z \in \enet}\|Az\| < 2c\sqrt{N}) \le e^{-CN} + |\enet|\cdot\max_{z \in \enet}\bP(\|Az\| \le 2c\sqrt{N})\\
& \le e^{-CN} + (3/\epsilon)^n(C_1 \cdot 2c)^N \le e^{-CN}+((3/\epsilon)^\lambda\cdot C_1\cdot 2c)^N\\
&\le e^{-CN} + (2C_1(3C_0/c)^\lambda c)^N \le e^{-CN} + e^{-CN}=2e^{-C N},
\end{split}
$$

for sufficiently small $c \in (0,C_0)$ such that $2C_1(3C_0/c)^\lambda c < e^{-C}$.

Therefore, given arbitrary $C>0$, the bound $Ps_\min(A) \le c\sqrt{N}) \le 2e^{-CN}$ is guaranteed by taking $c \in (0,c_\lambda(C))$, where
$$
\begin{split}
c_\lambda(C) := \min(C_0,(2C_1e^C(3C_0)^\lambda)^{\frac{-1}{1-\lambda}})>0.
\end{split}
\tag{2}
$$

This completes the proof of the claim. $\quad\quad\Box$

**Note.** An important highlight the above proof is that it works for every aspect ratio $\lambda \in (0,1)$.

## IV. -- Going beyond Gaussian matrices

A careful inspection of the proof of main ingredients **Fact 1** and **Fact 2**, reveals that we can replace the base distribution $N(0,1)$ of the coefficients of $A$ by any symmetric unit-variance $\sigma^2$-subGaussian such that $0 \le \sigma \le 1$.

  [1]: https://arxiv.org/pdf/1301.2382.pdf
  
*To be continued...*

